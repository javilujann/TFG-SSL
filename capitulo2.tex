\chapter{Introducción al Aprendizaje Semi-Supervisado}

En este capítulo se da un enfoque formal a los conceptos introducidos en el capítulo anterior. En la sección \ref{sec:problema_clasificacion} se presenta una definición formal de la clasificación semi-supervisada, el concepto sobre el que se articula el trabajo. Posteriormente, en la sección \ref{sec:suposiciones}, se presentan las suposiciones que hay detrás del aprendizaje semi-supervisado, y que son necesarias para entender cómo este puede obtener mejor rendimiento que el aprendizaje supervisado, y cuando es así.

\section{Problema de clasificación semi-supervisado}
\label{sec:problema_clasificacion}

Una vez introducida la idea en la que se basa el aprendizaje semi-supervisado, y dado que este trabajo se va a centrar en su aplicación a problemas de clasificación, es necesario definir de manera formal este problema. Para ello, vamos a hacer uso de una serie de definiciones que se encuentran en el libro de  \textit{Introduction to Semi-Supervised Learning, Xiaojin Zhu y Goldberg, 2009} \cite{zhu2009introduction}, y que se presentan a continuación.

Como primer paso se presenta la definición del aprendizaje supervisado, que, como se ha mencionado, es la base del aprendizaje semi-supervisado:

\begin{defi}[Aprendizaje Supervisado]
Sea $\mathcal{X}$ el dominio de las instancias, e $\mathcal{Y}$ el dominio de las etiquetas. Sea $P(\mathbf{x},y)$ una distribución de probabilidad conjunta (desconocida) sobre las instancias y etiquetas $\mathcal{X} \times \mathcal{Y}$. Dado un conjunto de entrenamiento $\{(\mathbf{x}_i, y_i)\}_{i=1}^n$ con $(\mathbf{x}_i, y_i) \overset{\text{i.i.d.}}{\sim} P(\mathbf{x},y)$ (independiente e idénticamente distribuido), el aprendizaje supervisado entrena, usando este conjunto, una función $f : \mathcal{X} \to \mathcal{Y}$ en alguna familia de funciones $\mathcal{F}$, con el objetivo de que $f(\mathbf{x})$ prediga la verdadera etiqueta $y$ en datos futuros $\mathbf{x}$, donde $(\mathbf{x},y) \overset{\text{i.i.d.}}{\sim} P(\mathbf{x},y)$ también.
\end{defi}

A partir de esta definición de aprendizaje supervisado, se puede entonces definir el problema de clasificación:

\begin{defi}[Clasificación]
La clasificación es el problema de aprendizaje supervisado con clases discretas $\mathcal{Y}$. La función $f$ se denomina clasificador.
\end{defi}

Finalmente, podemos definir el problema de clasificación semi-supervisado como una extensión del supervisado de la siguiente manera:

\begin{defi}[Clasificación Semi-Supervisada]
Bajo las condiciones del aprendizaje supervisado, se expande el conjunto de entrenamiento para que consista tanto en $l$ instancias etiquetadas $\{(\mathbf{x}_i, y_i)\}_{i=1}^l$, con $(\mathbf{x}_i, y_i) \overset{\text{i.i.d.}}{\sim} P(\mathbf{x},y)$,  como en $u$ instancias no etiquetadas $\{\mathbf{x}_j\}_{j=l+1}^{l+u}$, con $(\mathbf{x}_j) \overset{\text{i.i.d.}}{\sim} P(\mathbf{x})$, siendo este término la distribución marginal de $P(\mathbf{x},y)$ relativa al dominio de las instancias. El objetivo sigue siendo entrenar una función $f : \mathcal{X} \to \mathcal{Y}$ en alguna familia de funciones $\mathcal{F}$, con el objetivo de que $f(\mathbf{x})$ prediga la verdadera etiqueta $y$ en datos futuros $\mathbf{x}$, donde $(\mathbf{x},y) \overset{\text{i.i.d.}}{\sim} P(\mathbf{x},y)$.
\end{defi}

\begin{rema}
Se asume típicamente que hay muchos más datos no etiquetados que etiquetados, es decir, $u \gg l$.     
\end{rema}

\begin{rema}
Al tratarse del problema de clasificacion el dominio de las etiquetas $\mathcal{Y}$ es un conjunto discreto. Si fuera continuo estariamos hablando de un problema de regresión semi-supervisada que queda fuera del alcance de este trabajo.
\end{rema}

Con estas definiciones, queda establecido el marco teórico sobre el que se desarrolla este trabajo. Antes de cerrar este capítulo, se explicará por qué y bajo qué condiciones el aprendizaje semi-supervisado puede superar al aprendizaje supervisado. A partir de ahí, en los capítulos siguientes se presentará una taxonomía de los métodos de clasificación semi-supervisada y se analizarán en detalle los enfoques más relevantes.

\section{Suposiciones del aprendizaje semi-supervisado}
\label{sec:suposiciones}

Aunque no todos los métodos de aprendizaje semi-supervisado se basan explícitamente en probabilidades, resulta útil para entender cómo el aprendizaje semi-supervisado puede obtener mejor rendimiento que el aprendizaje supervisado, asumir que los métodos representan las hipótesis como $p(y \mid x)$ y los datos no etiquetados como $p(x)$. De esta manera, la información que proporciona $p(x)$ se puede introducir de dos posibles formas: o bien en modelos que estiman la distribución conjunta $p(x,y)$ que comparte parámetros con $p(x)$, o bien modificando la función objetivo para incluir términos de $p(x)$ \cite{zhu2005semi}.

Sin embargo, para que el conocimiento sobre la distribución de los datos de entrada $p(x)$ sea útil para inferir la etiqueta de salida $p(y \mid x)$, debe existir una relación estructural entre ambas. Es por esto que detrás de cada método de aprendizaje semi-supervisado hay una serie de suposiciones sobre cómo estas distribuciones se relacionan entre sí. Pero, antes de exponer estas suposiciones, conviene clarificar algunos conceptos fundamentales sobre la geometría de los datos:

\begin{itemize}
\item \textbf{Densidad:} Se refiere a la concentración de puntos en una región determinada del dominio de las instancias $\mathcal{X}$. Las regiones de alta densidad son aquellas donde el valor de $P(x)$ es alto, mientras que las de baja densidad tienen un valor bajo.
\item \textbf{Clúster:} Conjunto formado por una serie de puntos que, bajo una cierta distancia, se encuentran significativamente más cerca entre sí que del resto de los datos \cite{bishop2006pattern}.
\item \textbf{Frontera de decisión:} Es la superficie o hiperplano que divide el espacio de entrada $\mathcal{X}$ en regiones que se corresponden a diferentes categorías o clases \cite{bishop2006pattern}.
\item \textbf{Variedad (Manifold):} Se refiere a una estructura topológica de baja dimensión que se encuentra sumergida en un espacio de dimensión mayor.
\end{itemize}

Una vez presentados estos conceptos, se pueden entonces exponer cuáles son las suposiciones tal y como se presentan en \textit{Semi-supervised learning, Chapelle et al., 2006} \cite{10.7551/mitpress/9780262033589.001.0001}:

\begin{itemize}
    \item \textbf{Suposición de suavidad}: Si dos puntos $x_1$ y $x_2$ en una región de alta densidad están cerca, entonces también deben estarlo sus correspondientes salidas $y_1$ y $y_2$. 
    \item \textbf{Suposición de clústers}: Si los puntos se encuentran en un mismo clúster, es probable que tengan la misma etiqueta. 
    \item \textbf{Suposición de baja densidad}: La frontera de decisión debe pasar por regiones de baja densidad. Esta suposición es una reformulación de la suposición de clústers pues si los puntos de un mismo clúster tienen la misma etiqueta, entonces la frontera de decisión debe pasar por regiones de baja densidad donde no haya clústers.
    \item \textbf{Suposición de variedad subyacente}: Los datos se encuentran (aproximadamente) en una variedad de baja dimensión dentro de la representación de alta dimensión del espacio de entrada $\mathcal{X}.$
\end{itemize}

Destacar que si la suposición en la que se basa un metodo de aprendizaje semi-supervisado no se cumple, el uso de datos no etiquetados puede ser contraproducente y empeorar el rendimiento respecto al aprendizaje supervisado \cite{zhu2009introduction}.